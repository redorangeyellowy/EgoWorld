<!DOCTYPE html>
<html>
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    
    <meta property="og:title" content="EgoWorld: Translating Exocentric View to Egocentric View using Rich Exocentric Observations"/>
    <meta property="og:url" content="https://redorangeyellowy.github.io/EgoWorld/"/>
    <!-- <meta property="og:image" content="static/images/og_tag_header_image.jpg" /> -->
    <meta property="og:image:width" content="1200"/>
    <meta property="og:image:height" content="630"/>


    <title>EgoWorld</title>

    <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
    rel="stylesheet">
    <!-- <link rel="icon" href="static/images/icon.jpg"> -->

    <link rel="stylesheet" href="static/css/bulma.min.css">
    <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="static/css/bulma-slider.min.css">
    <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
    <link rel="stylesheet"
    href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
    <link rel="stylesheet" href="static/css/index.css">

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
    <script defer src="static/js/fontawesome.all.min.js"></script>
    <script src="static/js/bulma-carousel.min.js"></script>
    <script src="static/js/bulma-slider.min.js"></script>
    <script src="static/js/index.js"></script>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async
      src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>


<section class="publication-header">
  <div class="hero-body">
    <div class="container is-max-widescreen">
      <!-- <div class="columns is-centered"> -->
        <div class="column is-12 has-text-centered">
          <h1 class="title is-1 publication-title">EgoWorld:<br>Translating Exocentric View to Egocentric View<br>using Rich Exocentric Observations</h1>
          <h1 class="title is-3">arXiv 2025</h1>
        </div>
    </div>
  </div>   
</section>

<section class="publication-author-block">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column is-12 has-text-centered">
		      <div class="is-size-3 publication-authors">
          </div>
          
          <div class="is-size-3 publication-authors">
            <!-- span class="author-block"> </span> 
            <!-- <span class="eql-cntrb"><small><br><sup>*</sup>Indicates Equal Contribution</small></span> -->
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><a href="https://redorangeyellowy.github.io/" target="_blank">Junho Park</a><sup>1</sup>,</span>
            <span class="author-block"><a href="https://www.linkedin.com/in/andrew-sangwoo-ye-97a175199/" target="_blank">Andrew Sangwoo Ye</a><sup>2</sup>,</span>
            <span class="author-block"><a href="https://taeinkwon.com/" target="_blank">Taein Kwon</a><sup>3†</sup></span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>AI Lab, LG Electronics,</span>
            <span class="author-block"><sup>2</sup>KAIST,</span>
            <span class="author-block"><sup>3</sup>Visual Geometry Group, University of Oxford</span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block">(† : Corresponding author)</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              
              <span class="link-block">
                <a href="https://arxiv.org/abs/2506.17896" target="_blank" 
                class="external-link button is-normal is-rounded is-dark">
                <!-- <a target="_blank" class="external-link button is-normal is-rounded is-dark"> -->
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>

              <span class="link-block">
                <a href="https://github.com/redorangeyellowy/EgoWorld" target="_blank"
                class="external-link button is-normal is-rounded is-dark">
                <!-- <a target="_blank" class="external-link button is-normal is-rounded is-dark"> -->
                <span class="icon">
                  <i class="fab fa-github"></i>
                </span>
                <span>Code</span>
              </a>
              </span>

              <!-- <span class="link-block">
                <a href="https://redorangeyellowy.github.io/EgoWorld/" target="_blank"
                class="external-link button is-normal is-rounded is-dark">
                <span class="icon">
                  <i class="fab fa-youtube"></i>
                </span>
                <span>Video</span>
              </a>
              </span> -->
              
             <!--span class="link-block">
              <a href=""  target="_blank"
                 class="external-link button is-normal is-rounded">
                <span class="icon">
                    <i class="fas fa-infinity"></i>
                </span>
                <span>Colab</span>
              </a>
             </span -->
            
            <!-- <span class="link-block">
              <a href="https://huggingface.co/spaces/AttendAndExcite/Attend-and-Excite"  target="_blank"
                 class="external-link button is-normal is-rounded">
                <span class="icon">
                    <i class="fas fa-laptop"></i>
                </span>
                <span>Demo</span>
              </a>
             </span> -->
           
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- <section class="hero is-small"> -->
<section class="hero teaser">
  <!-- <div class="container is-max-desktop"> -->
  <div class="container is-max-widescreen">
    <div class="hero-body">
      <div class="container">
      <div class="item">
      <div class="column is-centered has-text-centered">
        <img src="static/images/teaser.png" alt="pipeline"/>
      <h2 class="subtitle">
        EgoWorld translates a single exocentric view into an egocentric view. 
        By leveraging rich multimodal exocentric observations, such as projected point clouds, 3D hand poses, and textual descriptions, EgoWorld is able to generate high-quality egocentric views, even in unseen scenarios. 
        Each observed modality provides complementary information that contributes to the accurate and realistic reconstruction of the egocentric view.
      </h2>
	  </div>
    </div>
  </div>
  </div>
  </div>
</section>

<section class="section hero is-light">
  <!-- <div class="container is-max-desktop"> -->
  <div class="container is-max-widescreen">
    <div class="columns is-centered has-text-centered">
      <!-- <div class="column is-four-fifths"> -->
      <div class="column is-12">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Egocentric vision is essential for both human and machine visual understanding, particularly in capturing the detailed hand-object interactions needed for manipulation tasks. 
            Translating third-person views into first-person views significantly benefits augmented reality (AR), virtual reality (VR) and robotics applications. However, current exocentric-to-egocentric translation methods are limited by their dependence on 2D cues, synchronized multi-view settings, and unrealistic assumptions such as necessity of initial egocentric frame and relative camera poses during inference.
            To overcome these challenges, we introduce EgoWorld, a novel two-stage framework that reconstructs an egocentric view from rich exocentric observations, including projected point clouds, 3D hand poses, and textual descriptions. Our approach reconstructs a point cloud from estimated exocentric depth maps, reprojects it into the egocentric perspective, and then applies diffusion-based inpainting to produce dense, semantically coherent egocentric images.
            Evaluated on the H2O and TACO datasets, EgoWorld achieves state-of-the-art performance and demonstrates robust generalization to new objects, actions, scenes, and subjects. 
            Moreover, EgoWorld shows promising results even on unlabeled real-world examples.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section hero">
  <div class="container is-max-widescreen">
    <div class="columns is-centered has-text-centered">
      <div class="column is-12">
        <h2 class="title is-3">Method</h2>
          <!-- <div class="column is-centered has-text-centered">
            <img src="static/images/method.png" alt="method" width="700px"/>
          </div> -->
          <!-- <div style="max-width: 100%; text-align: center; margin-bottom: 0;">
            <img src="static/images/method.png" alt="method" style="max-width: 100%; height: auto; margin-bottom: 0;">
          </div> -->
          <div style="width: 100%; text-align: center;">
            <img src="static/images/method.png" alt="method" style="max-width: 100%; height: auto; display: inline-block;">
          </div>
          <!-- <p class="content has-text-justified" style="margin-top: 0;"> -->
          <p class="content has-text-justified" style="margin-top: 1rem;">
            EgoWorld consists of two stages: exocentric view observation \( \Phi_{exo}$ \) and egocentric view reconstruction \( \Phi_{ego} \).
            First, given a single exocentric image \( {I}_{exo} \in \mathbb{R}^{H \times W \times 3} \), \( \Phi_{exo} \) predicts a corresponding sparse egocentric RGB map \( {S}_{ego} \in \mathbb{R}^{H \times W \times 3} \), 3D egocentric hand pose \( {P}_{ego} \in \mathbb{R}^{N \times 3} \), and a textual description \( T_{exo} \).
            \( H \) and \( W \) indicates height and width of \( {I}_{exo} \), and \( N \) indicates the number of keypoints of the hand.
            Then, in \( \Phi_{ego} \), an egocentric image \( \hat{I}_{ego} \in \mathbb{R}^{H \times W \times 3} \) is generated based on the observations predicted in \( \Phi_{exo} \).
            Therefore, EgoWorld is formulated as follows:
            \[
            {S}_{ego}, {P}_{ego}, T_{exo} = \Phi_{exo}({I}_{exo}),\\ 
            \hat{I}_{ego} = \Phi_{ego}({S}_{ego}, {P}_{ego}, T_{exo}).
            \]
          </p>
      </div>
    </div>
  </div>
</section> 

<section class="section hero is-light">
  <div class="container is-max-widescreen">
    <div class="columns is-centered has-text-centered">
      <div class="column is-12">
        <h2 class="title is-3">Real-World Generalization</h2>
          <div style="width: 100%; text-align: center;">
            <img src="static/images/exp_itw.png" alt="exp_itw" style="max-width: 100%; height: auto; display: inline-block;">
          </div>
          <p class="content has-text-justified" style="margin-top: 1rem;">
            We conduct experiments on EgoWorld with a state-of-the-art baseline model (e.g., CFLD) to evaluate in-the-wild generalization with unlabeled real-world examples.
            We take in-the-wild images of people interacting with arbitrary objects using their hands. 
            <b>Note that we rely solely on a single RGB image captured using a smartphone (iPhone 13 Pro) and apply our complete pipeline. No additional information beyond this single exocentric image is used.</b>
            As shown in the figure, CFLD produces egocentric images that appear unnatural, overly biased toward training images in H2O, and are inconsistent with the new interaction scenarios. 
            <b>In contrast, EgoWorld generates realistic, natural-looking egocentric views by effectively utilizing the sparse map, demonstrating strong generalization in unseen and real-world settings.</b>
            These results highlight EgoWorld’s robustness in in-the-wild scenarios, and with further training on diverse datasets, we believe it holds strong potential for real-world applications.
          </p>
      </div>
    </div>
  </div>
</section> 

<section class="section hero">
  <div class="container is-max-widescreen">
    <div class="columns is-centered has-text-centered">
      <div class="column is-12">
        <h2 class="title is-3">Comparisons on H2O</h2>
          <div style="width: 100%; text-align: center;">
            <img src="static/images/exp_comp_h2o.png" alt="exp_comp_h2o" style="max-width: 100%; height: auto; display: inline-block;">
          </div>
          <p class="content has-text-justified" style="margin-top: 1rem;">
            As illustrated in the figure, pix2pixHD produces egocentric images with noticeable noise, while pixelNeRF generates blurry outputs lacking fine details. 
            pix2pixHD, which relies on label map-based image-to-image translation, appears unsuitable for solving the exocentric-to-egocentric view translation problem. 
            Similarly, pixelNeRF is designed for novel view synthesis from multiple input views, making it less appropriate for the single-view to single-view translation task. 
            In contrast, CFLD effectively reconstructs the hand pose, but fails to translate detailed information about objects and scenes, often resulting in unrealistic objects or entirely unrelated backgrounds. 
            <b>In comparison, EgoWorld effectively leverages diverse information from the exocentric view, including pose maps, textual descriptions, and sparse maps, leading to robust performance even in challenging unseen scenarios involving complex elements like objects and scenes.</b>
          </p>
          <div style="width: 100%; text-align: center;">
            <img src="static/images/exp_comp_h2o_quan.png" alt="exp_comp_h2o_quan" style="max-width: 100%; height: auto; display: inline-block;">
          </div>
          <p class="content has-text-justified" style="margin-top: 1rem;">
            As shown in the table, pix2pixHD and pixelNeRF show poor performance in all scenarios.
            CFLD, which generates view-aware person image synthesis based on a given hand pose map, demonstrates stronger performance than pix2pixHD and pixelNeRf under view changes. 
            However, its capability is mostly limited to translating hand regions, and it performs poorly when it comes to reconstructing unseen regions such as objects and scenes. 
            <b>In contrast, EgoWorld successfully reconstructs information observed from the exocentric view in a manner that is coherent and natural in the egocentric perspective, and outperforms all unseen scenarios in all metrics compared to state-of-the-arts.</b>
          </p>
      </div>
    </div>
  </div>
</section> 

<section class="section hero is-light">
  <div class="container is-max-widescreen">
    <div class="columns is-centered has-text-centered">
      <div class="column is-12">
        <h2 class="title is-3">Comparisons on TACO</h2>
          <div style="width: 100%; text-align: center;">
            <img src="static/images/exp_comp_taco.png" alt="exp_comp_taco" style="max-width: 100%; height: auto; display: inline-block;">
          </div>
          <p class="content has-text-justified" style="margin-top: 1rem;">
            As illustrated in the figure, EgoWorld demonstrates strong generalization performance even on TACO, which contains a wide variety of objects and actions compared to H2O. 
            Unlike CFLD, which struggles to reconstruct information beyond the hand region, <b>EgoWorld shows a remarkable ability to restore not only the hand but also the interacting objects and surrounding scene.</b>
            These results confirm that EgoWorld is capable of delivering robust performance across diverse domains. 
          </p>
      </div>
    </div>
  </div>
</section> 

<section class="section hero">
  <div class="container is-max-widescreen">
    <div class="columns is-centered has-text-centered">
      <div class="column is-12">
        <h2 class="title is-3">Effect of Conditioning Modalities</h2>
          <div style="width: 100%; text-align: center;">
            <img src="static/images/exp_guide.png" alt="exp_guide" style="max-width: 100%; height: auto; display: inline-block;">
          </div>
          <p class="content has-text-justified" style="margin-top: 1rem;">
            EgoWorld reconstructs faithful egocentric images based on both the pose map and the textual description. 
            As illustrated in the figure, the absence of text leads to incorrect reconstructions of unseen objects. 
            In contrast, when text is available, the textual object information predicted from the exocentric image is effectively reflected in the egocentric view reconstruction, resulting in more plausible outputs. 
            Additionally, the presence of hand pose information allows EgoWorld to produce hand configurations closer to the ground truth. 
            These validate that EgoWorld performs best when leveraging both pose and textual observations. 
          </p>
      </div>
    </div>
  </div>
</section> 

<section class="section hero is-light">
  <div class="container is-max-widescreen">
    <div class="columns is-centered has-text-centered">
      <div class="column is-12">
        <h2 class="title is-3">Backbone of Image Completion</h2>
          <div style="width: 100%; text-align: center;">
            <img src="static/images/exp_recon.png" alt="exp_recon" style="max-width: 100%; height: auto; display: inline-block;">
          </div>
          <p class="content has-text-justified" style="margin-top: 1rem;">
            Since egocentric view reconstruction closely resembles the image completion task, we compare our method with state-of-the-art image completion backbones, such as Masked AutoEncoder (MAE), Mask-Aware Transformer (MAT), and Latent Diffusion Model (LDM). 
            Specifically, MAE is specialized in mask-based image encoding, making it effective for filling missing pixel regions. 
            MAT, a transformer-based model, excels at restoring large missing areas through long-range context modeling. 
            LDM, serving as the baseline for EgoWorld, differs from the others in its ability to condition on diverse modalities such as text and pose. 
            As shown in the figure, our LDM-based method reconstructs egocentric view images in a more natural and high-quality manner compared to other methods. 
            Although the vanilla MAT model performs well in filling missing areas, it often struggles to maintain consistency with the surrounding content. 
            For example, subtle differences in table color are noticeable. 
            To address this, we develop a refined version of MAT that uses random patch masking and recovery. 
            However, this approach tends to fail in preserving detailed local interactions, such as hand-object interaction. 
            In contrast, our LDM-based method, which operates by adding and removing noise in latent space, achieves coherent restoration not only in local regions but also in preserving consistency with existing areas. 
            Therefore, based on these results, we adopt LDM as the backbone for EgoWorld.
          </p>
      </div>
    </div>
  </div>
</section> 

<section class="section hero">
  <div class="container is-max-widescreen">
    <div class="columns is-centered has-text-centered">
      <div class="column is-12">
        <h2 class="title is-3">3D Egocentric Hand Pose Estimation from Exocentric View</h2>
          <div style="width: 100%; text-align: center;">
            <img src="static/images/exp_egopose.png" alt="exp_egopose" style="max-width: 100%; height: auto; display: inline-block;">
          </div>
          <p class="content has-text-justified" style="margin-top: 1rem;">
            To validate the effectiveness of our newly proposed exocentric image-based 3D egocentric hand pose estimator, we conduct a qualitative analysis. 
            As shown in the figure, given a single exocentric view image as input, our model predicts 3D hand poses that closely resemble the ground truth. 
            This demonstrates that the estimator is highly useful in the exocentric view observation stage for calculating the translation matrix, as well as in the egocentric view reconstruction stage for initializing the hand pose map.
          </p>
      </div>
    </div>
  </div>
</section> 

<section class="section hero is-light">
  <div class="container is-max-widescreen">
    <div class="columns is-centered has-text-centered">
      <div class="column is-12">
        <h2 class="title is-3">Incorrect Textual Description Guidance</h2>
          <div style="width: 100%; text-align: center;">
            <img src="static/images/exp_incrt.png" alt="exp_incrt" style="max-width: 100%; height: auto; display: inline-block;">
          </div>
          <p class="content has-text-justified" style="margin-top: 1rem;">
            To evaluate the effect of textual description guidance of the egocentric view reconstruction, we intentionally provide an incorrect textual description that does not match the exocentric image. 
            As shown in the figure, the object in the egocentric view is generated to match the object described in the description. 
            From this result, we observe two key insights: (1) the final egocentric image can vary depending on the output of the VLM, highlighting the importance of the VLM’s performance; and (2) even when arbitrary exocentric images are fed, our model performed sufficient generalization to unseen scenarios. 
          </p>
      </div>
    </div>
  </div>
</section> 

<section class="section hero">
  <div class="container is-max-widescreen">
    <div class="columns is-centered has-text-centered">
      <div class="column is-12">
        <h2 class="title is-3">Generation Consistency</h2>
          <div style="width: 100%; text-align: center;">
            <img src="static/images/exp_consi.png" alt="exp_consi" style="max-width: 100%; height: auto; display: inline-block;">
          </div>
          <p class="content has-text-justified" style="margin-top: 1rem;">
            To evaluate the consistency of our generative model, we generated egocentric images multiple times under identical conditions. 
            As shown in the figure, we present four outputs generated from the same exocentric image and corresponding sparse map, and our model consistently produces coherent egocentric images across runs. 
            Despite the inherent variability in generative models, our method achieves stable and reliable exocentric-to-egocentric view translation, demonstrating its robustness and consistency.
          </p>
      </div>
    </div>
  </div>
</section> 


<section class="section hero" id="BibTeX">
  <div class="container is-max-widescreen">
    <h2 class="title">BibTeX</h2>
    <div style="max-width: 100%; overflow-x: auto;">
      <pre style="margin: 0; white-space: pre-wrap; word-break: break-all;"><code>@article{park2025egoworld,
  author    = {Park, Junho and Ye, Andrew Sangwoo and Kwon, Taein},
  title     = {EgoWorld: Translating Exocentric View to Egocentric View using Rich Exocentric Observations},
  journal   = {arXiv preprint arXiv:2506.17896},
  year      = {2025},
}</code></pre>
    </div>
  </div>
</section>

<footer class="footer">
  <div class="columns is-centered">
    <div class="column is-8">
      <div class="content">
        <p>
          This website is licensed under a <a rel="license"
          href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
        Commons Attribution-ShareAlike 4.0 International License</a>.
      </p>
      <p>
        Website source code based on the <a href="https://nerfies.github.io/"> Nerfies</a> project page. If you want to reuse their <a
        href="https://github.com/nerfies/nerfies.github.io">source code</a>, please credit them appropriately.
      </p>
    </div>
  </div>
</div>
</div>
</footer>


  <script type="text/javascript">
    var sc_project=12351448; 
    var sc_invisible=1; 
    var sc_security="c676de4f"; 
  </script>
  <script type="text/javascript"
  src="https://www.statcounter.com/counter/counter.js"
  async></script>
  <noscript><div class="statcounter"><a title="Web Analytics"
    href="https://statcounter.com/" target="_blank"><img
    class="statcounter"
    src="https://c.statcounter.com/12351448/0/c676de4f/1/"
    alt="Web Analytics"></a></div></noscript>
    <!-- End of Statcounter Code -->

  </body>
  </html>